{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "from openpyxl import workbook, load_workbook\n",
    "from openpyxl.utils import get_column_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of user-agent strings to choose from\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36 Edg/91.0.864.59\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/89.0.2\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/89.0.2\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 11; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 11; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36 EdgA/46.03.4.5155\",\n",
    "    \"Mozilla/5.0 (Linux; Android 11; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/89.0.2\",\n",
    "]\n",
    "\n",
    "#Access Excel sheet data\n",
    "wb = load_workbook('PC_component_list.xlsx')\n",
    "\n",
    "#Specify scraping time delay\n",
    "min_delay = 0\n",
    "max_delay = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run consecutive functions to pull 9 pages of product URLs & fill an empty list\n",
    "product_url_list = []\n",
    "\n",
    "def pull_product_url_page_1():\n",
    "    #Use URL of page 1\n",
    "    url = 'https://geizhals.at/?cat=mbp4_1200#productlist'\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    div_elements = soup.find_all('div', class_='cell productlist__item productlist__name')\n",
    "\n",
    "    for div in div_elements:\n",
    "        a_element = div.find('a')\n",
    "        if a_element and 'href' in a_element.attrs:\n",
    "            href_value = a_element['href']\n",
    "            product_url_list.append(f\"https://geizhals.at/{href_value}\")\n",
    "\n",
    "def pull_product_url(page_num, max_pages):\n",
    "    if page_num > max_pages:\n",
    "        return\n",
    "    #Use URL of page 2, as {page_num} can then be dynamic\n",
    "    base_url = 'https://geizhals.at/?cat=mbp4_1200&pg={page_num}#productlist'\n",
    "    url = base_url.format(page_num=page_num)\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    div_elements = soup.find_all('div', class_='cell productlist__item productlist__name')\n",
    "\n",
    "    for div in div_elements:\n",
    "        a_element = div.find('a')\n",
    "        if a_element and 'href' in a_element.attrs:\n",
    "            href_value = a_element['href']\n",
    "            product_url_list.append(f\"https://geizhals.at/{href_value}\")\n",
    "\n",
    "    next_page_num = page_num + 1\n",
    "    delay = random.uniform(min_delay, max_delay)\n",
    "    time.sleep(delay)\n",
    "    pull_product_url(next_page_num, max_pages)\n",
    "\n",
    "\n",
    "pull_product_url_page_1()\n",
    "pull_product_url(2, max_pages=4)\n",
    "print(product_url_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Excel database with product URLs\n",
    "\n",
    "ws = wb['NEW']\n",
    "for index, url in enumerate(product_url_list, start=2):\n",
    "    cell = ws.cell(row=index, column=2, value=url)\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape gpu data\n",
    "\n",
    "def gpu_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = data[0].text\n",
    "    memory = data[1].text\n",
    "    dimensions = data[12].text\n",
    "    connections = data[7].text\n",
    "    first_release = data[29].text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    return title, component_price, image, memory, dimensions, connections, first_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape cpu data\n",
    "\n",
    "def cpu_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    soup.find_all('meta')\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    graphics = soup.find_all('dd')[5].text\n",
    "    base_clock = soup.find_all('dd')[3].text\n",
    "    boost_clock = soup.find_all('dd')[2].text\n",
    "    socket = soup.find_all('dd')[6].text\n",
    "    ddr = soup.find_all('dd')[13].text\n",
    "    pcie = soup.find_all('dd')[21].text\n",
    "    introduced = soup.find_all('dd')[23].text\n",
    "    return component_price, image, graphics, base_clock, boost_clock, socket, ddr, pcie, introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape mobo data\n",
    "\n",
    "def mobo_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    form_factor = data[0].text\n",
    "    socket = data[1].text\n",
    "    chipset = data[2].text\n",
    "    ram = data[4].text\n",
    "    internal_headers = data[7].text\n",
    "    buttons = data[10].text\n",
    "    audio = data[11].text\n",
    "    graphics = data[12].text\n",
    "    wireless = data[13].text\n",
    "\n",
    "    return title, component_price, image, form_factor, socket, chipset, ram, internal_headers, buttons, audio, graphics, wireless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape RAM data\n",
    "\n",
    "def ram_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    ddr = data[0].text\n",
    "    rate = data[1].text\n",
    "    module = data[2].text\n",
    "    rgb = data[11].text\n",
    "\n",
    "    return title, component_price, image, ddr, rate, module, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape SSD data\n",
    "\n",
    "def ssd_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    form_factor = data[1].text\n",
    "    interface = data[2].text\n",
    "    read = data[3].text\n",
    "    write = data[4].text\n",
    "\n",
    "    return title, component_price, image, form_factor, interface, read, write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape House data\n",
    "\n",
    "def house_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    front_io = data[2].text\n",
    "    front_fan = data[4].text\n",
    "    rear_fan = data[5].text\n",
    "    top_fan = data[8].text\n",
    "    radiator = data[11].text\n",
    "    mobo = data[13].text\n",
    "    psu = data[14].text\n",
    "    cpu_cooler = data[16].text\n",
    "    gpu_length = data[17].text\n",
    "    colour = data[18].text\n",
    "    rgb = data[19].text\n",
    "    description = data[31].text\n",
    "\n",
    "    return title, component_price, image, front_io, front_fan, rear_fan, top_fan, radiator, mobo, psu, cpu_cooler, gpu_length, colour, rgb, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape AIO data\n",
    "\n",
    "def aio_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    heatsink = data[2].text\n",
    "    socket = data[3].text\n",
    "    length = data[4].text\n",
    "    thickness = data[5].text\n",
    "    fans = data[7].text\n",
    "    pump = data[9].text\n",
    "\n",
    "    return title, component_price, image, heatsink, socket, length, thickness, fans, pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape CPU fan data\n",
    "\n",
    "def cpu_fan_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "        title = soup.find('h1').text\n",
    "        component_price = soup.find_all('meta')[3]['content']\n",
    "        image = soup.find_all('meta')[2]['content']\n",
    "\n",
    "        data = soup.find_all('dd')\n",
    "        dimensions = data[1].text if len(data) > 1 else \"N/A\"\n",
    "        socket_amd = data[5].text if len(data) > 5 else \"N/A\"\n",
    "        socket_intel = data[6].text if len(data) > 6 else \"N/A\"\n",
    "        wattage = data[7].text if len(data) > 7 else \"N/A\"\n",
    "        rgb = data[8].text if len(data) > 8 else \"N/A\"\n",
    "        extra = data[9].text if len(data) > 9 else \"N/A\"\n",
    "\n",
    "        return title, component_price, image, dimensions, socket_amd, socket_intel, wattage, rgb, extra\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching {url}: {e}\")\n",
    "        return None, None, None, None, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to scrape PSU data\n",
    "\n",
    "def psu_data_gather(url):\n",
    "    user_agent = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    data = soup.find_all('dd')\n",
    "    title = soup.find('h1').text\n",
    "    component_price = soup.find_all('meta')[3]['content']\n",
    "    image = soup.find_all('meta')[2]['content']\n",
    "    one = data[1].text\n",
    "    two = data[2].text\n",
    "    three = data[3].text\n",
    "    four = data[4].text\n",
    "    five = data[5].text\n",
    "    six = data[6].text\n",
    "    seven = data[7].text\n",
    "    eight = data[8].text\n",
    "    nine = data[9].text\n",
    "    ten = data[10].text\n",
    "    eleven = data[11].text\n",
    "    twelve = data[12].text\n",
    "    thirteen = data[13].text\n",
    "    fourteen = data[14].text\n",
    "    fifteen = data[15].text\n",
    "    sixteen = data[16].text\n",
    "    seventeen = data[17].text\n",
    "    eighteen = data[18].text\n",
    "\n",
    "    return one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing GPU Excel sheet data\n",
    "\n",
    "def update_gpu_data():\n",
    "    ws = wb['GPU']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=451, min_col=1, max_col=8):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, memory, dimensions, connections, first_release = gpu_data_gather(component_url)\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = memory\n",
    "        row[4].value = dimensions\n",
    "        row[5].value = connections\n",
    "        row[6].value = first_release\n",
    "        row[7].value = image\n",
    "        print(title, first_release)\n",
    "\n",
    "update_gpu_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cpu_data():\n",
    "    ws = wb['CPU']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=70, min_col=1, max_col=11):\n",
    "        component_name = row[0].value\n",
    "        component_url = row[1].value\n",
    "        component_price, image, graphics, base_clock, boost_clock, socket, ddr, pcie, introduced = cpu_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[2].value = component_price\n",
    "        row[3].value = socket\n",
    "        row[4].value = graphics\n",
    "        row[5].value = base_clock\n",
    "        row[6].value = boost_clock\n",
    "        row[7].value = pcie\n",
    "        row[8].value = ddr\n",
    "        row[9].value = introduced\n",
    "        row[10].value = image\n",
    "        print(component_name, component_price, base_clock, socket)\n",
    "\n",
    "update_cpu_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mobo_data():\n",
    "    ws = wb['MOBO']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=340, min_col=1, max_col=13):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, form_factor, socket, chipset, ram, internal_headers, buttons, audio, graphics, wireless = mobo_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = socket\n",
    "        row[4].value = chipset\n",
    "        row[5].value = ram\n",
    "        row[6].value = form_factor\n",
    "        row[7].value = internal_headers\n",
    "        row[8].value = buttons\n",
    "        row[9].value = audio\n",
    "        row[10].value = graphics\n",
    "        row[11].value = wireless\n",
    "        row[12].value = image\n",
    "        print(title, component_price, socket, wireless)\n",
    "\n",
    "update_mobo_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ram_data():\n",
    "    ws = wb['RAM']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=68, min_col=1, max_col=8):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, ddr, rate, module, rgb = ram_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = ddr\n",
    "        row[4].value = rate\n",
    "        row[5].value = module\n",
    "        row[6].value = rgb\n",
    "        row[7].value = image\n",
    "        print(title)\n",
    "        print(rate)\n",
    "\n",
    "update_ram_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ssd_data():\n",
    "    ws = wb['SSD']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=51, min_col=1, max_col=9):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, form_factor, interface, read, write = ssd_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = form_factor\n",
    "        row[4].value = interface\n",
    "        row[6].value = read\n",
    "        row[7].value = write\n",
    "        row[8].value = image\n",
    "        \n",
    "        print(title)\n",
    "        print(form_factor)\n",
    "        print(interface)\n",
    "        print(read)\n",
    "        print(write)\n",
    "\n",
    "update_ssd_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_psu_data():\n",
    "    ws = wb['PSU']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=78, min_col=1, max_col=30):\n",
    "        component_url = row[1].value\n",
    "        one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen = psu_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[12].value = one\n",
    "        row[13].value = two\n",
    "        row[14].value = three\n",
    "        row[15].value = four\n",
    "        row[16].value = five\n",
    "        row[17].value = six\n",
    "        row[18].value = seven\n",
    "        row[19].value = eight\n",
    "        row[20].value = nine\n",
    "        row[21].value = ten\n",
    "        row[22].value = eleven\n",
    "        row[23].value = twelve\n",
    "        row[24].value = thirteen\n",
    "        row[25].value = fourteen\n",
    "        row[26].value = fifteen\n",
    "        row[27].value = sixteen\n",
    "        row[28].value = seventeen\n",
    "        row[29].value = eighteen\n",
    "\n",
    "update_psu_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_aio_data():\n",
    "    ws = wb['AIO']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=421, min_col=1, max_col=10):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, heatsink, socket, length, thickness, fans, pump = aio_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = heatsink\n",
    "        row[4].value = socket\n",
    "        row[5].value = length\n",
    "        row[6].value = thickness\n",
    "        row[7].value = fans\n",
    "        row[8].value = pump\n",
    "        row[9].value = image\n",
    "        \n",
    "        print(title)\n",
    "        print(heatsink)\n",
    "        print(length)\n",
    "        print(pump)\n",
    "\n",
    "update_aio_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_house_data():\n",
    "    ws = wb['House']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=151, min_col=1, max_col=16):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, front_io, front_fan, rear_fan, top_fan, radiator, mobo, psu, cpu_cooler, gpu_length, colour, rgb, description = house_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = front_io\n",
    "        row[4].value = front_fan\n",
    "        row[5].value = rear_fan\n",
    "        row[6].value = top_fan\n",
    "        row[7].value = radiator\n",
    "        row[8].value = mobo\n",
    "        row[9].value = psu\n",
    "        row[10].value = cpu_cooler\n",
    "        row[11].value = gpu_length\n",
    "        row[12].value = colour\n",
    "        row[13].value = rgb\n",
    "        row[14].value = description\n",
    "        row[15].value = image\n",
    "        \n",
    "        print(title)\n",
    "        print(mobo)\n",
    "        print(colour)\n",
    "        print(description)\n",
    "\n",
    "update_house_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cpu_fan_data():\n",
    "    ws = wb['CPU_fan']\n",
    "    for row in ws.iter_rows(min_row=2, max_row=241, min_col=1, max_col=10):\n",
    "        component_url = row[1].value\n",
    "        title, component_price, image, dimensions, socket_amd, socket_intel, wattage, rgb, extra = cpu_fan_data_gather(component_url)\n",
    "\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "        row[0].value = title\n",
    "        row[2].value = component_price\n",
    "        row[3].value = dimensions\n",
    "        row[4].value = socket_amd\n",
    "        row[5].value = socket_intel\n",
    "        row[6].value = wattage\n",
    "        row[7].value = rgb\n",
    "        row[8].value = image\n",
    "        row[9].value = extra\n",
    "        \n",
    "        print(title)\n",
    "        print(dimensions)\n",
    "        print(wattage)\n",
    "\n",
    "update_cpu_fan_data()\n",
    "wb.save('PC_component_list_updated.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
